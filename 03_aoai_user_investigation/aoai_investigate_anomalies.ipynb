{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AOAI Insider Threat Analysis\n",
        "This notebook leverages Azure OpenAI (AOAI) to simulate the reasoning of a cybersecurity analyst. Once users have been nominated as anomalous (e.g., via the train_isolation_forest.ipynb script), this notebook is used to investigate and explain their behavior using log summaries and engineered features.\n",
        "\n",
        "##### Approach\n",
        "_Log types: device, email, file, HTTP, logon logs_\n",
        "\n",
        "For a user:\n",
        "1) **Log Retrieval:** Pull logs from each data source for a 60-day window ending at the user’s most recent activity. \n",
        "\n",
        "2) **Log Summarization:** For each log source, the logs are broken into manageable chunks. Azure Open AI analyzes and summarizes each chunk to identify unusual access patterns or suspicious events and assigns relevance scores based on event severity. Azure OpenAI then synthesizes the chunk-level summaries into a single summary per log source, highlighting the most critical behaviors.  \n",
        "\n",
        "3) **Final Report Generation:** The synthesized summaries are combined with user background information and engineered features to produce a structured report. The report includes a behavior summary, timeline of anomalous events, risk assessment, and recommended next steps. \n",
        "\n",
        "#### Before Running This Notebook\n",
        "- Update \"api_key\" with your Azure OpenAI API Key\n",
        "- Set the \"user\" variable to the username you want to investigate\n",
        "- Set the \"log_window_days\" variable to the desired X number of recent days you would like to analyze. I suggest matching the time range the isolation forest anomaly detection was performed on.\n",
        "- Ensure the cleaned log tables (device, email, file, logon, http) are accessible\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "from openai import AzureOpenAI\n",
        "from IPython.display import Markdown, display\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE THIS to the username you would like to investigate\n",
        "user = \"XXXXXXXX-ID\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to AOAI\n",
        "client = AzureOpenAI(\n",
        "    api_key= \"AOAI_API_KEY\", # REPLACE WITH YOUR KEY\n",
        "    api_version=\"2025-01-01-preview\",\n",
        "    azure_endpoint=\"https://cyber-aoai.openai.azure.com/\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1) Log Retreival"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE THIS to your desired time window\n",
        "log_window_days = 60"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the date range for the last {log_window_days} of activity for the user\n",
        "# This is how far in the past the log data will be pulled / investigation analysis will be performed\n",
        "def get_last_date(table_name, user, date_col=\"date\"):\n",
        "    query = f\"SELECT MAX({date_col}) AS last_date FROM {table_name} WHERE user = '{user}'\"\n",
        "    return spark.sql(query).collect()[0][\"last_date\"]\n",
        "\n",
        "device_last = get_last_date(\"clean_device_events\", user)\n",
        "email_last = get_last_date(\"clean_email_events\", user)\n",
        "file_last = get_last_date(\"clean_file_events\", user)\n",
        "logon_last = get_last_date(\"clean_logon_events\", user)\n",
        "http_last = get_last_date(\"clean_http_events\", user)\n",
        "\n",
        "# get most recent date\n",
        "all_dates = [d for d in [device_last, email_last, file_last, logon_last, http_last] if d is not None]\n",
        "most_recent_date = max(all_dates)\n",
        "\n",
        "# compute day range\n",
        "start_window = most_recent_date - pd.Timedelta(days=log_window_days)\n",
        "\n",
        "\n",
        "start_date_str = start_window.strftime(\"%Y-%m-%d\")\n",
        "end_date_str = most_recent_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "print(f\"User: {user}\")\n",
        "print(f\"Most recent activity date: {start_date_str}\")\n",
        "print(f\"{log_window_days}-day window: {start_date_str} to {end_date_str}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2) Log Summarization (with chunking)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking set up\n",
        "chunk_size = 500\n",
        "max_tokens_per_chunk = 1000"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Log source queries with descriptions of what is contained in the dataset and what to look for in analysis\n",
        "log_sources = {\n",
        "    \"device\": {\n",
        "        \"query\": f\"\"\"\n",
        "            SELECT date, user, pc, activity\n",
        "            FROM clean_device_events\n",
        "            WHERE user = '{user}' AND date BETWEEN DATE('{start_date_str}') AND DATE('{end_date_str}')\n",
        "            ORDER BY date ASC\n",
        "        \"\"\",\n",
        "        \"columns\": [\"date\", \"pc\", \"activity\"],\n",
        "        \"description\": \"Device logs capture USB thumb drive connect/disconnect events. \\\n",
        "        Some disconnects may be missing due to power-downs.\\\n",
        "        Deviations from a user's normal usage may indicate data exfiltration.\"\n",
        "    },\n",
        "    \"email\": {\n",
        "        \"query\": f\"\"\"\n",
        "            SELECT date, user, pc, to, cc, bcc, from, size, attachments, content\n",
        "            FROM clean_email_events\n",
        "            WHERE user = '{user}' AND date BETWEEN DATE('{start_date_str}') AND DATE('{end_date_str}')\n",
        "            ORDER BY date ASC\n",
        "        \"\"\",\n",
        "        \"columns\": [\"date\", \"pc\", \"from\", \"to\", \"cc\", \"bcc\", \"attachments\", \"content\"],\n",
        "        \"description\": \"Email logs include sender/recipient metadata and content. \\\n",
        "        External recipients (non-DTAA email addresses) with large attachments may suspicious. \\\n",
        "        Content is keyword-based and not tied to subject/body.\"\n",
        "    },\n",
        "    \"file\": {\n",
        "        \"query\": f\"\"\"\n",
        "            SELECT date, user, pc, filename, content\n",
        "            FROM clean_file_events\n",
        "            WHERE user = '{user}' AND date BETWEEN DATE('{start_date_str}') AND DATE('{end_date_str}')\n",
        "            ORDER BY date ASC\n",
        "        \"\"\",\n",
        "        \"columns\": [\"date\", \"pc\", \"filename\", \"content\"],\n",
        "        \"description\": \"File logs represent file copies to removable media. \\\n",
        "        Content includes file headers and keywords. \\\n",
        "        Deviations from normal copy volume or sensitive filenames may be suspicious.\"\n",
        "    },\n",
        "    \"logon\": {\n",
        "        \"query\": f\"\"\"\n",
        "            SELECT date, user, pc, activity\n",
        "            FROM clean_logon_events\n",
        "            WHERE user = '{user}' AND date BETWEEN DATE('{start_date_str}') AND DATE('{end_date_str}')\n",
        "            ORDER BY date ASC\n",
        "        \"\"\",\n",
        "        \"columns\": [\"date\", \"pc\", \"activity\"],\n",
        "        \"description\": \"Logon logs include logon/logoff events. \\\n",
        "        Screen unlocks are recorded as logons. Screen locks are not recorded. \\\n",
        "        Deviations from normal after-hours (outside of 6 AM to 6 PM) and weekend logon activity may be suspicious\", \n",
        "    },\n",
        "    \"http\": {\n",
        "        \"query\": f\"\"\"\n",
        "            SELECT date, user, url, pc\n",
        "            FROM clean_http_events\n",
        "            WHERE user = '{user}' AND date BETWEEN DATE('{start_date_str}') AND DATE('{end_date_str}')\n",
        "            ORDER BY date ASC\n",
        "        \"\"\",\n",
        "        \"columns\": [\"date\", \"pc\", \"url\", \"content\"],\n",
        "        \"description\": \"HTTP logs capture web browsing activity. \\\n",
        "        Visits to risky domains or domains linked to malware/keylogging may indicate insider threat behavior.\"\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk DataFrame\n",
        "def chunk_dataframe(df, size):\n",
        "    return [df.iloc[i:i + size] for i in range(0, len(df), size)]\n",
        "\n",
        "# summarize a single chunk\n",
        "def summarize_chunk(log_type, chunk_df, chunk_index, total_chunks, description):\n",
        "    chunk_text = \"\\n\".join([\" | \".join(str(row[col]) for col in chunk_df.columns) for _, row in chunk_df.iterrows()])\n",
        "    prompt = f\"\"\"\n",
        "        You are a cybersecurity analyst. Analyze the following {log_type} logs (Chunk {chunk_index + 1} of {total_chunks}).\n",
        "        {description}\n",
        "\n",
        "        Instructions:\n",
        "        - Summarize key behaviors or anomalies.\n",
        "        - Flag any log entries that may indicate insider threat activity.\n",
        "        - For each flagged entry, include the timestamp and a relevance score (1-5).\n",
        "        - Keep your response under {max_tokens_per_chunk} tokens.\n",
        "\n",
        "        Logs:\n",
        "        {chunk_text}\n",
        "        \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a cybersecurity analyst assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=max_tokens_per_chunk\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# summarize all chunks for a log dataset\n",
        "def summarize_log_type(log_type):\n",
        "    log_info = log_sources[log_type]\n",
        "    df = spark.sql(log_info[\"query\"]).toPandas().drop(columns=[\"user\"])\n",
        "    chunks = chunk_dataframe(df, chunk_size)\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        summary = summarize_chunk(log_type, chunk, i, len(chunks), log_info[\"description\"])\n",
        "        summaries.append(summary)\n",
        "    combined_prompt = f\"\"\"\n",
        "        You are a cybersecurity analyst. Below are summaries of {log_type} log chunks for user {user}.\n",
        "        Combine them into a single summary highlighting the most important findings, flagged entries, and behavioral patterns.\n",
        "\n",
        "        Summaries:\n",
        "        {chr(10).join(summaries)}\n",
        "        \"\"\"\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a cybersecurity analyst assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": combined_prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "    return final_response.choices[0].message.content"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# chunk and synthesize summaries for each log type\n",
        "# this may take a few minutes depending on number of logs\n",
        "# try altering the chunk size to speed up summarization\n",
        "\n",
        "device_summary = summarize_log_type(\"device\")\n",
        "email_summary = summarize_log_type(\"email\")\n",
        "file_summary = summarize_log_type(\"file\")\n",
        "logon_summary = summarize_log_type(\"logon\")\n",
        "http_summary = summarize_log_type(\"http\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3) Final Report Generation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get users engineered features\n",
        "# (created from engineer_model_features.ipynb, stored in model_features table)\n",
        "features_df = spark.sql(f\"\"\"\n",
        "SELECT *\n",
        "FROM model_features\n",
        "WHERE user = '{user}'\n",
        "\"\"\").toPandas()\n",
        "if not features_df.empty:\n",
        "    features = features_df.iloc[0].to_dict()\n",
        "else:\n",
        "    features = {}\n",
        "\n",
        "# Get users description\n",
        "# (LDAP details from clean_user_details.ipynb, stored in clean_user_details table)\n",
        "# includes employee background on users role, supervisor, etc\n",
        "user_details_df = spark.sql(f\"\"\"\n",
        "SELECT *\n",
        "FROM clean_user_details\n",
        "WHERE user = '{user}'\n",
        "\"\"\").toPandas()\n",
        "if not user_details_df.empty:\n",
        "    user_details = user_details_df.iloc[0].to_dict()\n",
        "else:\n",
        "    user_details = {}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Build prompt\n",
        "def build_prompt(user_id, user_details, features, device_summary, email_summary, file_summary, logon_summary, http_summary):\n",
        "    return f\"\"\"\n",
        "    You are a cybersecurity analyst assistant. Your task is to analyze user behavior to assess the risk of \n",
        "    insider threat activity over a {log_window_days} time range from {start_date_str} to {end_date_str}. \n",
        "    Use behavioral patterns, anomalies, and semantic cues to support your assessment. \n",
        "    Write in a concise, analytical tone. Focus on:\n",
        "    - Patterns over time (e.g., spikes, shifts, or anomalies)\n",
        "    - Deviations from baseline behavior\n",
        "    - Malicious activity\n",
        "    -----\n",
        "\n",
        "    ## User Profile\n",
        "    User ID: {user_id}\n",
        "    User Background Information: {str(user_details)}\n",
        "\n",
        "    ## User Activity Features: {features}\n",
        "    The user activity features are derived from raw data sources such as device logs, email logs, file logs, and HTTP logs. \n",
        "    These features capture significant patterns and anomalies in user behavior that may indicate potential risks. \n",
        "    The key components include:\n",
        "    - variables starting with \"recent_\" capture user behavior in the 14 days leading up to their last recorded event.\n",
        "    This reflects short-term activity and is crucial for detecting pre-departure anomalies.\n",
        "    - variables starting with \"baseline_\" capture typical user behavior in the 60 days prior to the recent window. \n",
        "    - variables ending with \"_spike_ratio\" compare recent vs. baseline activity to quantify unusual surges\n",
        "\n",
        "    ## Log activity summaries\n",
        "    These log analysis summaries are compiled by reviewing log data over {start_date_str} to {end_date_str} time range.\n",
        "\n",
        "    Recent Device Event Summary:\n",
        "    {device_summary}\n",
        "\n",
        "    Recent Email Events Summary:\n",
        "    {email_summary}\n",
        "\n",
        "    Recent File Events Summary:\n",
        "    {file_summary}\n",
        "\n",
        "    Recent Logon Events Summary:\n",
        "    {logon_summary}\n",
        "\n",
        "    Recent HTTP Events Summary:\n",
        "    {http_summary}\n",
        "\n",
        "    -------\n",
        "\n",
        "    Use the structured template below to summarize your findings.\n",
        "\n",
        "    ## Analysis Output Template\n",
        "\n",
        "    **User Summary**\n",
        "    User: [Full Name] ({user_id}) — [1-sentence overview based on background]\n",
        "    Time Window Analyzed: {start_date_str} to {end_date_str}\n",
        "\n",
        "    **Behavior Summary**\n",
        "    [1-3 sentence summary of the user's recent behavior, highlighting any shifts or patterns]\n",
        "\n",
        "    **Anomalous Activities** [sort most anomalous/malicious to least] \\n\n",
        "    1. [High level description of anomalous activity] : [1 sentence of events or behavior] \\n\n",
        "    2. [High level description of anomalous activity] : [1 sentence of events or behavior] \\n\n",
        "    ....[add more if necessary]\n",
        "\n",
        "    **Anomalous Timeline of Events** [sort most recent to oldest] \\n\n",
        "    - [Date Range 1] : [1-3 sentence of events or behavior] \\n\n",
        "    - [Date Range 2] : [1-3 sentence of events or behavior] \\n\n",
        "    ....[add more if necessary]\n",
        "\n",
        "    **Risk Assessment**\n",
        "    - Risk Level: [Low / Medium / High]\n",
        "    - Justification: [Brief explanation based on data]\n",
        "\n",
        "    **Recommendations**\n",
        "    - [Suggested next steps: e.g., escalate, monitor, interview, etc.]\n",
        "\n",
        "    ---\n",
        "\n",
        "    Only use the data provided. Do not fabricate or assume information not present in the log summaries or features. \n",
        "    Provide examples or strong justifications. Only include activities/analysis you are certain of.\n",
        "    When providing URL links from the log summaries, make sure they are not clickable. \n",
        "    Don't include user emotions, focus on technical facts.\n",
        "    \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit prompt to AOAI - with user engineered features, employee LDAP info, and log summaries\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4.1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a cybersecurity analyst assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": build_prompt(user, user_details, features, device_summary, email_summary, file_summary, logon_summary, http_summary)}\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=1000\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Display analysis results\n",
        "aoai_output = response.choices[0].message.content\n",
        "display(Markdown(aoai_output))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}