{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## engineer_model_features\n",
        "\n",
        "This notebook extracts user behavior features from five types of activity logs (device, email, file, HTTP, and logon) to support insider threat detection using an anomaly detection model (see: investigate_anomalies.ipynb).\n",
        "\n",
        "#### üß† Feature Engineering Strategy\n",
        "\n",
        "**Temporal windowing approach to capture behavioral shifts:** \n",
        "* 14-day recent window: Captures user behavior in the 14 days leading up to their last recorded event. This reflects short-term activity and is crucial for detecting pre-departure anomalies.\n",
        "* 60-day baseline window: Captures typical user behavior in the 60 days prior to the recent window. This helps establish a personalized behavioral norm.\n",
        "\n",
        "This strategy enables us to detect deviations from a user's baseline ‚Äî a key signal for insider threats, especially for users who may engage in risky actions shortly before exiting the organization.\n",
        "\n",
        "#### üõ†Ô∏è Features Include:\n",
        "* **Volume metrics:** email_sent_count, logon_count, http_request_count, etc\n",
        "* **Behavioral flags:** after_hours_logon, after_hours_file_access, to_external_email_count, etc\n",
        "* **Uniqueness metrics:** unique_files_count, unique_url_count, etc\n",
        "* **Spike ratios**: compare recent vs. baseline activity to quantify unusual surges (ex. logon_spike_ratio, file_access_spike_ratio)\n",
        "\n",
        "This combination of temporal and behavioral features helps the model differentiate between normal variation and potential insider threats.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from functools import reduce"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# set up\n",
        "WORK_HOURS_START = 6\n",
        "WORK_HOURS_END = 18\n",
        "is_after_hours = (F.hour(\"date\") < WORK_HOURS_START) | (F.hour(\"date\") >= WORK_HOURS_END)\n",
        "is_weekend = F.dayofweek(\"date\").isin([1,7])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "device_df = spark.read.table(\"clean_device_events\")\n",
        "email_df = spark.read.table(\"clean_email_events\")\n",
        "file_df = spark.read.table(\"clean_file_events\")\n",
        "http_df = spark.read.table(\"clean_http_events\")\n",
        "logon_df = spark.read.table(\"clean_logon_events\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all events and determine users first and last activity date\n",
        "all_events = device_df.select(\"user\",\"date\") \\\n",
        "            .union(email_df.select(\"user\",\"date\")) \\\n",
        "            .union(file_df.select(\"user\",\"date\")) \\\n",
        "            .union(http_df.select(\"user\",\"date\")) \\\n",
        "            .union(logon_df.select(\"user\",\"date\")) \n",
        "\n",
        "user_activity_window = all_events.groupBy(\"user\").agg(\n",
        "    F.min(\"date\").alias(\"first_seen\"),\n",
        "    F.max(\"date\").alias(\"last_seen\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dynamic 14-day recent and 60-day baseline window per user\n",
        "# 14 day window - most recent 14 days of activity for that user (most recent activity)\n",
        "# 60 day window - 60 days prior to the 7 day window (baselines users activity) \n",
        "user_activity_window = all_events.groupBy(\"user\").agg(\n",
        "    F.min(\"date\").alias(\"first_seen\"),\n",
        "    F.max(\"date\").alias(\"last_seen\")\n",
        ")\n",
        "\n",
        "user_activity_window = user_activity_window.withColumn(\"14d_recent_window_start\", F.expr(\"last_seen - interval 14 days\")) \\\n",
        "                                    .withColumn(\"60d_baseline_window_end\", F.expr(\"14d_recent_window_start - interval 1 day\")) \\\n",
        "                                    .withColumn(\"60d_baseline_window_start\", F.expr(\"60d_baseline_window_end - interval 60 days\"))\n",
        "\n",
        "user_window_broadcast = F.broadcast(user_activity_window)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recent_and_baseline(df):\n",
        "    df = df.join(user_window_broadcast, on=\"user\")\n",
        "    recent_df = df.filter((F.col(\"date\") >= F.col(\"14d_recent_window_start\")) & (F.col(\"date\") <= F.col(\"last_seen\")))\n",
        "    baseline_df = df.filter((F.col(\"date\") >= F.col(\"60d_baseline_window_start\")) & (F.col(\"date\") <= F.col(\"60d_baseline_window_end\")))\n",
        "    return recent_df, baseline_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# DEVICE FEATURES\n",
        "device_recent_df, device_baseline_df = get_recent_and_baseline(device_df)\n",
        "device_recent_df = device_recent_df.groupBy(\"user\").agg(F.count(\"*\").alias(\"recent_device_count\"))\n",
        "device_baseline_df = device_baseline_df.groupBy(\"user\").agg(F.count(\"*\").alias(\"baseline_device_count\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# EMAIL FEATURES\n",
        "email_recent_df, email_baseline_df = get_recent_and_baseline(email_df)\n",
        "email_recent_df = email_recent_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"recent_email_sent_count\"),\n",
        "        F.avg(\"size\").alias(\"recent_avg_email_size\"),\n",
        "        F.avg(\"attachments\").alias(\"recent_avg_attachment_size\"),\n",
        "        F.count(F.when(~F.col(\"from\").rlike(\"@dtaa\\\\.com$\"), True)).alias(\"recent_from_external_count\"),\n",
        "        F.count(F.when(~F.col(\"to\").rlike(\"@dtaa\\\\.com$\"), True)).alias(\"recent_to_external_count\"),\n",
        ")\n",
        "\n",
        "email_baseline_df = email_baseline_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"baseline_email_sent_count\"),\n",
        "        F.avg(\"size\").alias(\"baseline_avg_email_size\"),\n",
        "        F.avg(\"attachments\").alias(\"baseline_avg_attachment_size\"),\n",
        "        F.count(F.when(~F.col(\"from\").rlike(\"@dtaa\\\\.com$\"), True)).alias(\"baseline_from_external_count\"),\n",
        "        F.count(F.when(~F.col(\"to\").rlike(\"@dtaa\\\\.com$\"), True)).alias(\"baseline_to_external_count\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE FEATURES\n",
        "file_recent_df, file_baseline_df = get_recent_and_baseline(file_df)\n",
        "file_recent_df = file_recent_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"recent_file_access_count\"),\n",
        "        F.countDistinct(\"filename\").alias(\"recent_unique_file_count\")\n",
        ")\n",
        "\n",
        "file_baseline_df = file_baseline_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"baseline_file_access_count\"),\n",
        "        F.countDistinct(\"filename\").alias(\"baseline_unique_file_count\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# HTTP FEATURES\n",
        "risky_keywords = [\"wikileaks\", \"keylogger\", \"malware\", \"malicious\",\"exploit\",\"leak\"]\n",
        "risky_pattern = \"|\".join(risky_keywords)\n",
        "\n",
        "http_recent_df, http_baseline_df = get_recent_and_baseline(http_df)\n",
        "http_recent_df = http_recent_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"recent_http_request_count\"),\n",
        "        F.countDistinct(\"url\").alias(\"recent_unique_url_count\"),\n",
        "        F.count(F.when(F.col(\"url\").rlike(risky_pattern), True)).alias(\"recent_risky_url_count\")\n",
        "\n",
        ")\n",
        "\n",
        "http_baseline_df = http_baseline_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"baseline_http_request_count\"),\n",
        "        F.countDistinct(\"url\").alias(\"baseline_unique_url_count\"),\n",
        "        F.count(F.when(F.col(\"url\").rlike(risky_pattern), True)).alias(\"baseline_risky_url_count\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGON FEATURES\n",
        "logon_recent_df, logon_baseline_df = get_recent_and_baseline(logon_df)\n",
        "logon_recent_df = logon_recent_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"recent_logon_count\"),\n",
        "        F.sum(F.when(is_after_hours, 1).otherwise(0)).alias(\"recent_after_hours_logon\")\n",
        ")\n",
        "\n",
        "logon_baseline_df = logon_baseline_df.groupBy(\"user\").agg(\n",
        "        F.count(\"*\").alias(\"baseline_logon_count\"),\n",
        "        F.sum(F.when(is_after_hours, 1).otherwise(0)).alias(\"baseline_after_hours_logon\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Join feature tables\n",
        "def join_all_dfs(dfs, join_col=\"user\"):\n",
        "    return reduce(lambda df1, df2: df1.join(df2, on=join_col, how=\"outer\"), dfs)\n",
        "\n",
        "final_features = join_all_dfs([\n",
        "    device_recent_df, device_baseline_df,\n",
        "    email_recent_df, email_baseline_df,\n",
        "    file_recent_df, file_baseline_df,\n",
        "    http_recent_df, http_baseline_df,\n",
        "    logon_recent_df, logon_baseline_df\n",
        "])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "final_features = final_features.withColumns({\n",
        "    \"device_spike_ratio\" : F.col(\"recent_device_count\") / (F.col(\"baseline_device_count\") + 1),\n",
        "    \"file_spike_ratio\" : F.col(\"recent_file_access_count\") / (F.col(\"baseline_file_access_count\") + 1),\n",
        "    \"email_spike_ratio\" : F.col(\"recent_email_sent_count\") / (F.col(\"baseline_email_sent_count\") + 1),\n",
        "    \"http_spike_ratio\" : F.col(\"recent_http_request_count\") / (F.col(\"baseline_http_request_count\") + 1),\n",
        "    \"logon_spike_ratio\" : F.col(\"recent_logon_count\") / (F.col(\"baseline_logon_count\") + 1)\n",
        "})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "final_features.write.mode(\"overwrite\").saveAsTable(\"model_features\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}